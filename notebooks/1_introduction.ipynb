{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aea1ec1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Simone-Alghisi/HMD-Lab/blob/master/notebooks/1_introduction.ipynb)\n",
    "\n",
    "On Colab:\n",
    "1. Switch to a GPU Runtime by clicking on *Runtime > Change runtime type > T4 GPU*\n",
    "2. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fe831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab only\n",
    "!git clone https://github.com/Simone-Alghisi/HMD-Lab.git\n",
    "%cd /content/HMD-Lab/notebooks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495cbb3",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38fc30",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "After [installing the required components](./0_installation.ipynb), you can interact with the project using:\n",
    "```shell\n",
    "python -m main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875ab8a",
   "metadata": {},
   "source": [
    "## Prompting (In-Context Learning)\n",
    "\n",
    "A prompt is natural language sequence to condition the generation of the model (prefixes for the Language Model).\n",
    "\n",
    "We Zero-Shot, One-Shot, and Few-Shot prompt the model to obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/miniconda3/envs/hmd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import MODELS\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name, InitModel, prepare_text = MODELS[\"qwen3\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = InitModel(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from notebooks.notebook_utils import display_conversation\n",
    "from models.qwen3 import prepare_text\n",
    "\n",
    "\n",
    "task_prompt = \"Answer using as few words as possible. Answer to this question by considering the Examples provided.\"\n",
    "user_message = \"What's 6+6?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690bc11",
   "metadata": {},
   "source": [
    "### Zero-Shot\n",
    "\n",
    "Usually, Zero-Shot prompts contain \n",
    "1. (optionally) a description of the task, which should specify the model what to do (e.g., *\"Summarize this text\"*)\n",
    "2. the input for the current model (e.g., the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30ca49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Conversation\n",
       "\n",
       "**System:** Answer using as few words as possible. Answer to this question by considering the Examples provided.\n",
       "\n",
       "**User:** What's 6+6?\n",
       "\n",
       "**Assistant:** 12"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zero_shot = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": task_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "text = prepare_text(user_message, tokenizer, zero_shot)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=16384).cpu()\n",
    "\n",
    "# decode the output\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()\n",
    "content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "display_conversation(zero_shot, user_message, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0ec04",
   "metadata": {},
   "source": [
    "### One-Shot\n",
    "\n",
    "One-Shot prompts can be seen as an extension of Zero-Shot, where we also include an example for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55615b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Conversation\n",
       "\n",
       "**System:** Answer using as few words as possible. Answer to this question by considering the Examples provided.\n",
       "\n",
       "Example: What's 4+4? 8\n",
       "\n",
       "**User:** What's 6+6?\n",
       "\n",
       "**Assistant:** 12"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_shot = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": task_prompt + \"\\n\\nExample: What's 4+4? 8\"\n",
    "    },\n",
    "]\n",
    "\n",
    "text = prepare_text(user_message, tokenizer, one_shot)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=16384).cpu()\n",
    "\n",
    "# decode the output\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()\n",
    "content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "display_conversation(one_shot, user_message, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdadf2b",
   "metadata": {},
   "source": [
    "### Few-Shots\n",
    "\n",
    "A Few-Shots prompt can be seen as a One-Shot prompt, with more than one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db705e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Conversation\n",
       "\n",
       "**System:** Answer using as few words as possible. Answer to this question by considering the Examples provided.\n",
       "\n",
       "Example:\n",
       "- What's 4+4? 6\n",
       "- What's 1+1? 6\n",
       "- What's 2+2? 6\n",
       "- What's 3+3? 10\n",
       "\n",
       "**User:** What's 6+6?\n",
       "\n",
       "**Assistant:** 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "few_shot = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": task_prompt + \"\\n\\nExample:\\n- What's 4+4? 6\\n- What's 1+1? 6\\n- What's 2+2? 6\\n- What's 3+3? 10\"\n",
    "    },\n",
    "]\n",
    "\n",
    "text = prepare_text(user_message, tokenizer, few_shot)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=16384).cpu()\n",
    "\n",
    "# decode the output\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()\n",
    "content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "display_conversation(few_shot, user_message, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63feab09",
   "metadata": {},
   "source": [
    "## Classification with LLMs\n",
    "\n",
    "LLMs can be used as zero-shot or few-shot classification by prompting them to return a label or short answer. This approach is convenient for quick experiments or when labeled data is scarce, but keep in mind: outputs may require post-processing to map natural-language answers to fixed labels.\n",
    "\n",
    "Considerations:\n",
    "- Prefer short, explicit answer formats (e.g., \"Answer: <label>\").\n",
    "- Use few-shot examples to disambiguate label wording.\n",
    "- Validate on a held-out set and apply simple normalization (lowercasing, trimming) to model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de0175c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Conversation\n",
       "\n",
       "**System:** You are a concise classifier. Classify the sentiment of the following text. Choose one label: positive, negative\n",
       "\n",
       "**User:** Text: \"I really enjoyed the movie. The acting was superb.\"\n",
       "Answer:\n",
       "\n",
       "**Assistant:** positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_to_classify = \"I really enjoyed the movie. The acting was superb.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a concise classifier. Classify the sentiment of the following text. Choose one label: positive, negative\"\n",
    "    }\n",
    "]\n",
    "user_message = \"Text: \\\"\" + text_to_classify + \"\\\"\\nAnswer:\"\n",
    "\n",
    "text = prepare_text(user_message, tokenizer, messages)\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(**inputs, max_new_tokens=32).cpu()\n",
    "\n",
    "output_ids = generated[0][len(inputs.input_ids[0]) :].tolist()\n",
    "out = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "# Simple normalization to match labels\n",
    "out = out.strip().lower().split()[0]\n",
    "display_conversation(messages, user_message, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e207d",
   "metadata": {},
   "source": [
    "## Data generation with LLMs\n",
    "\n",
    "LLMs are useful to generate synthetic data for training or augmenting datasets. Use prompts that explicitly request the format you want (JSON, CSV-like lines, or label + text). Validate generated samples and filter or deduplicate before using them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40aded3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"This film is a breathtaking blend of stunning visuals, compelling storytelling, and powerful performances. The director's vision comes through clearly, and the emotional depth resonates long after the credits roll.\",\n",
      "  \"label\": \"positive\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"The film suffers from a lackluster plot and underdeveloped characters, making it difficult to stay engaged throughout. The pacing is sluggish, and the dialogue feels forced and unnatural.\",\n",
      "  \"label\": \"negative\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"This film is a breathtaking blend of emotion and stunning visuals. The storytelling is compelling, and the performances are heartfelt and authentic. A must-watch for anyone who loves deeply moving cinema.\",\n",
      "  \"label\": \"positive\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"The film suffers from poor pacing and uninspired storytelling, making it difficult to stay engaged throughout. The characters feel flat, and the plot lacks any meaningful development.\",\n",
      "  \"label\": \"negative\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "def gen_sample(prompt, sentiment):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"You output a single JSON object with keys 'text' and 'label'.\"\n",
    "        }\n",
    "    ]\n",
    "    text = prepare_text(prompt.format(sentiment), tokenizer, messages)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(**inputs, max_new_tokens=128).cpu()\n",
    "    out_ids = ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "\n",
    "prompt_template = (\n",
    "    \"Generate a short movie review and a sentiment label. Output exactly one JSON object like:\\n\"\n",
    "    \"{{\\\"text\\\": \\\"...\\\", \\\"label\\\": \\\"{}\\\"}}\\n\\n\"\n",
    "\n",
    ")\n",
    "\n",
    "n = 4\n",
    "samples = []\n",
    "for i in trange(n):\n",
    "    sentiment = \"positive\" if i%2==0 else \"negative\"\n",
    "    raw = gen_sample(prompt_template, sentiment)\n",
    "    try:\n",
    "        obj = json.loads(raw.strip())\n",
    "        samples.append(obj)\n",
    "    except Exception:\n",
    "        print(\"Failed to parse generated sample:\\n\", raw)\n",
    "\n",
    "for s in samples:\n",
    "    print(json.dumps(s, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4bf65",
   "metadata": {},
   "source": [
    "## End-to-end OrderBot\n",
    "\n",
    "You will now modify the code in  [main.py](../main.py) to act as an End-to-End OrderBot.\n",
    "\n",
    "1. You can change the content on the file by using `nano main.py` on your remote machine\n",
    "2. Change the \"content\" of the first message to include the prompt below\n",
    "3. Interact with the system to understand its capabilities (`python -m main`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You are OrderBot, an automated service to collect orders for a pizza restaurant.\n",
    "You first greet the customer, then collects the order,\n",
    "and then asks if it's a pickup or delivery.\n",
    "You wait to collect the entire order, then summarize it and check for a final\n",
    "time if the customer wants to add anything else.\n",
    "If it's a delivery, you ask for an address.\n",
    "Finally you collect the payment.\n",
    "Make sure to clarify all options, extras and sizes to uniquely\n",
    "identify the item from the menu.\n",
    "You respond in a short, very conversational friendly style.\n",
    "The menu includes:\n",
    "pepperoni pizza 12.95, 10.00, 7.00\n",
    "cheese pizza 10.95, 9.25, 6.50\n",
    "eggplant pizza 11.95, 9.75, 6.75\n",
    "fries 4.50, 3.50\n",
    "greek salad 7.25\n",
    "Toppings: \\\\ extra cheese 2.00, \\\\ mushrooms 1.50 \\\\ sausage 3.00 \\\\ canadian bacon 3.50\n",
    "AI sauce 1.50 \\\\ peppers 1.00\n",
    "Drinks: \\\\ coke 3.00, 2.00, 1.00 \\\\ sprite 3.00, 2.00, 1.00 \\\\ bottled water 5.00\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
